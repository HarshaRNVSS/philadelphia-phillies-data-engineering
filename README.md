# Phillies Pitch-Level Data Engineering Assessment

## Overview

This project implements a Python-based data pipeline to ingest, clean, validate, enrich, and aggregate high-frequency pitch tracking data from a baseball game. The goal is to transform nested JSON pitch data into an analytics-ready dataset and compute batter-level performance metrics using SQL.

The pipeline is designed to be defensive against missing or malformed data, easy to run and reproduce, and suitable for downstream analytics and research workflows.

---

## GitHub Repository

The complete source code for this project is available on GitHub:

ðŸ”— **[https://github.com/HarshaRNVSS/philadelphia-phillies-data-engineering](https://github.com/HarshaRNVSS/philadelphia-phillies-data-engineering)**

## Project Structure

```text
PhiladelphiaPhilliesProject/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ batch_raw.json              # Raw pitch tracking input
â”‚   â”œâ”€â”€ processed_pitches.parquet   # Cleaned, pitch-level dataset
â”‚   â””â”€â”€ batter_summary.csv          # Batter-level aggregated metrics
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ transform.py                # Data cleaning & feature engineering
â”‚   â”œâ”€â”€ main.py                     # Pipeline entry point
â”‚   â””â”€â”€ aggregate.py                # SQL aggregation using DuckDB
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_transform.py           # Unit test for transformation logic
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ venv/
```
---
## Environment Setup
This project uses a Python virtual environment.

### Step 1: Create a virtual environment
``` text
python3 -m venv venv
```
### Step 2: Activate the virtual environment
``` text
source venv/bin/activate
```
You should see (venv) in your terminal prompt.
### Step 3: Install dependencies
``` text
python -m pip install -r requirements.txt
```
---
## Running the pipeline
All commands should be run from the project root directory with the virtual environment activated.

### Step 1: Transform raw pitch data
``` text
python src/main.py
```
This step:
	â€¢	Loads data/batch_raw.json
	â€¢	Flattens nested JSON so each row represents one pitch
	â€¢	Validates and cleans key fields
	â€¢	Enriches the dataset with derived features
	â€¢	Writes the processed output to:
``` text
data/processed_pitches.parquet
```
### Step 2: Aggregate batter-level metrics
```text
python src/aggregate.py
```
This step:
	â€¢	Loads the processed pitch-level dataset
	â€¢	Uses DuckDB (in-memory SQL engine) for aggregation
	â€¢	Produces one row per batter
	â€¢	Writes results to:
```text
data/batter_summary.csv
```
Metrics computed:
	â€¢	Swing count
	â€¢	Whiff rate (percentage of swings with no contact)
	â€¢	Maximum exit velocity (mph)
---
## Running Unit Tests
A basic unit test is included to validate core transformation logic.
From the project root, run:
```text
pytest
```
Expected output:
```text
1 passed
```
---
## Key Assumptions & Design Choices
	â€¢	One JSON object represents one pitch
	â€¢	Missing events or personId values are allowed; batter_id is set to null
	â€¢	A swing is defined by the presence of non-empty samples_bat
	â€¢	Contact is defined as a swing with a valid exit velocity
	â€¢	Exit velocity values are validated using realistic baseball constraints (0â€“125 mph)
	â€¢	Rows are not dropped due to missing data; invalid values are safely nulled
	â€¢	DuckDB is used for SQL aggregation to keep analytics logic simple and transparent

Rows with missing batter_id are grouped under NULL during aggregation, reflecting incomplete tracking data rather than data loss.
---
## Technologies Used
	â€¢	Python 3
	â€¢	pandas
	â€¢	DuckDB
	â€¢	PyArrow (Parquet)
	â€¢	pytest
---
## Outputs
	â€¢	processed_pitches.parquet â€” clean, pitch-level dataset suitable for analytics
	â€¢	batter_summary.csv â€” batter-level performance summary

